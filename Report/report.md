# 四子棋实验

## 实验目标

编写一个四子棋的AI，能够在限定时间内进行决策，通过自动对战对 AI 的水平进行测试。

## 目录结构

- Compete: 助教提供的命令行对战客户端
- Report: 实验报告
- Strategy: AI 代码目录
- UI: 助教提供的基于 .NET 的界面对战客户端
- Testcases: 助教提供的测试 AI
- Tests: 本机上与测试 AI 对战的结果

主要代码在 Strategy 目录下，Tests 目录下记录了对战结果。

## 测试平台

由于只提供了 32bit 的 macOS 上的二进制问卷，选择了一台比较老的机器，配置如下：

- MacBookAir Early 2015
- CPU: Intel Core i5-5250U @ 1.60GHz
- OS: macOS 10.13.6 17G6030
- Kernel: x86_64 Darwin 177.0

本机性能比较低，所以最终AI的对战结果可能和在助教机器上测得结果不同。

## 算法思路

在 Strategy 目录下实现的算法是信息上限树算法（UCT），基本按照如下伪代码对照实现，另外做了一些针对四子棋的优化。伪代码如下：

```
    function UCTSEARCH(S_0)
        以状态s_0创建根节点v_0;
        while 尚未用完计算时长 do:
            v_l←TREEPOLICY(v_0);
            ∆←DEFAULTPOLICY(s(v_l));
            BACKUP(v_l,∆);
        end while
        return a(BESTCHILD(v_0,0));

    function TREEPOLICY(v)
        while 节点v不是终止节点 do:
            if 节点v是可扩展的 then:
                return EXPAND(v)
            else:
                v← BESTCHILD(v,c)
            return v

    function EXPAND(v)
        选择行动a∈A(state(v))中尚未选择过的行动
        向节点v添加子节点v'，使得s(v')= f(s(v),a),a(v')=a
        return v'

    function BESTCHILD(v,c)
        return 〖argmax〗_(v'∈children of v) ((Q(v'))/(N(v'))+c√((2ln(N(v)))/(N(v'))))

    function DEFAULTPOLICY(s)
        while s不是终止状态 do:
            以等概率选择行动a∈A(s)
            s←f(s,a)
        return 状态s的收益

    function BACKUP(v,Δ)
        while v≠NULL do:
            N(v)←N(v)+1
            Q(v)←Q(v)+∆
            ∆←1-∆
            v←v的父节点
```

大概思路就是，首先通过一定的策略找到一个需要扩展的节点，然后从这个节点开始随机模拟一次对战，把这次对战的结果更新到从这个新节点到根节点的路径上。

搜索到一定思考时间后，选择根节点中胜率最高的子节点，作为 AI 目前的决策。

## 算法实现

### UCTSEARCH

代码在 `UCT.cpp` 中 `UCT::Search` 中。这个函数初始化棋盘，然后不断循环进行节点的扩展、模拟和更新，直到时间耗尽。

每次循环中，通过 `treePolicy` 找到需要扩展的节点，然后调用 `defaultPolicy` 得到一次模拟的胜率，通过 `backup` 来把这一次模拟的数据更新到这个节点到根节点的路径。

其中时间的获取通过 `gettimeofday` ，目前参数采用的是 2.9s ，即只要计算超过 2.9s 就停止。

最后从根节点中选择一个胜率最高的子节点，然后释放内存并退出。

### TREEPOLICY

代码在 `UCT.cpp` 的 `UCT::treePolicy`中。这个函数的功能是从根节点往下找到一个需要扩展的节点。如果当前节点是终止节点，就停止；否则要么扩展出新的一个节点，要么从已有的子节点中选择一个权值最高的节点。

### EXPAND

在 `UCTNode::expandOne` 中实现。它随机选取一个可以下的位置，新建一个新的节点并且返回。

### BESTCHILD

实现在 `UCTNode::bestChild` 函数中。它找出子节点中权值最高的那一个并且返回。这个函数兼任了两个功能，一个是 treePolicy 中的选择，一个是根节点寻找胜率最高的节点。

这里权值的选取就是 UCB 算法中上限信心索引的计算公式 `I_j = X_j + \sqrt{\frac{2ln(n)}{T_j(n)}`。实际在使用的时候，需要加一个权值，代码中使用的是 1.0 。

对于寻找根节点的最优子节点的时候，总会出现本来只剩一步就可以胜利了，但在必胜的时候仍然在其它地方下子，这样会很浪费评测的时间。于是在遇到必胜策略的时候，优先选择直接胜利的那一步。这在对战很弱的对手的时候有较好的效果。

早就必胜还在为难对手的例子：

![](2019-05-12-11-18-31.png)

### DEFAULTPOLICY

代码在 `UCT::defaultPolicy` 中。它实现功能是，从当前节点开始，模拟一场对局。按照原来的解法，这里是每一步都随机直到最后决出胜负。但是实践上出现了问题。

最突出的问题就是会突然“暴毙”，特别是到了接近结尾的时候。仔细研究了以后发现，由于棋盘接近下慢，出现的可能性很小了，有的情况下，按照胜率计算并不准确，例如 67% 的胜率是在对方有三个位置可以走，两个输一个赢的情况下算出来的，但对方不傻，一定会选择赢的那一种，所以这个时候计算的胜率不再正确。为了解决这个问题，在模拟的时候，进行了一步必胜的考虑，分为以下情况：

- 如果己方下某一个位置就可以获胜，那么直接按照胜利处理。
- 如果上一条不成立，但对方下两个或以上位置都可以胜利，那么直接按照失败处理。
- 如果己方下了某一个位置没有获胜，但对方可以立即在同一列上下子获胜，那么直接按照失败处理。

通过这些策略，“暴毙”的情况得到了一定的减少。

### BACKUP

实现在 `UCTNode::backup` 中。它的功能就是从当前节点向上走，不断更新模拟的总数量和胜率。由于双方是交替下棋，所以胜率要在0和1之间交替计算。

这样就可以把整棵树的胜率计算出来了。

## 对战结果

通过运行 `Compete` 将编写的 AI 和若干个 Testcase 进行了对打。评测结果都通过 `make compete opponent=xxx` 运行得到，输出写到 `Tests` 目录下。文件命名 `test-A-100` 代表编写的 AI 作为 A 方与 100 号 Testcase 对战的结果。由于每次对战都要花很多时间，所以得到的结果比较少。

最后得到了如下的并不精确的胜率数据：

| 测试样例 | 作为A方的胜场/总场 | 作为B方的胜场/总场 |
| -------- | ------------------ | ------------------ |
| 2        | 2/2                | 2/2                |
| 10       | 10/10              | 10/10              |
| 50       | 2/2                | 2/2                |
| 80       | 3/10               | 4/10               |
| 90       | 1/2                | 1/2                |
| 94       | 1/2                | 1/2                |
| 96       | 0/2                | 1/2                |
| 98       | 7/20               | 6/20               |
| 100      | 7/20               | 9/20               |




## 后续改进

对 UCT 的改进主要有两个方面，一个是提高速度，增加模拟的次数，这可以通过优化一些耗时的操作来达到，例如用位运算来进行胜利条件的判断，用内存池代替 new 和 delete 等等；另一个是优化 `defaultPolicy` 的剪枝，把一些显而易见的最优步骤直接计算出来，这样可以减少上面见到的“暴毙”的问题。

当然了，也可以换别的算法来实现这个 AI ，例如用非常优秀的评价函数来写 Minimax 算法等等。